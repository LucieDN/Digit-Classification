{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digits classification with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installations and importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\della\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB 660.6 kB/s eta 0:00:17\n",
      "   ---------------------------------------- 0.1/11.0 MB 812.7 kB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.1/11.0 MB 819.2 kB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.1/11.0 MB 819.2 kB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.1/11.0 MB 819.2 kB/s eta 0:00:14\n",
      "    --------------------------------------- 0.2/11.0 MB 655.4 kB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.3/11.0 MB 850.6 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.3/11.0 MB 827.2 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.3/11.0 MB 838.1 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.3/11.0 MB 838.1 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.4/11.0 MB 716.8 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.5/11.0 MB 842.4 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.5/11.0 MB 862.0 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.6/11.0 MB 846.7 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.6/11.0 MB 863.3 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.6/11.0 MB 868.8 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.7/11.0 MB 873.8 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.7/11.0 MB 843.9 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.7/11.0 MB 858.3 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.0 MB 862.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.9/11.0 MB 725.2 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.9/11.0 MB 713.6 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.9/11.0 MB 728.0 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/11.0 MB 733.1 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/11.0 MB 738.4 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.1/11.0 MB 742.9 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.1/11.0 MB 754.1 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.1/11.0 MB 750.9 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.2/11.0 MB 754.6 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.2/11.0 MB 759.2 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.2/11.0 MB 756.2 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 765.5 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 769.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/11.0 MB 773.6 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.4/11.0 MB 770.4 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.4/11.0 MB 778.6 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 775.5 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 776.7 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 780.1 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 781.5 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 783.5 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.7/11.0 MB 785.5 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.7/11.0 MB 788.4 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.8/11.0 MB 794.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.8/11.0 MB 791.9 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.8/11.0 MB 798.0 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.9/11.0 MB 790.0 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.9/11.0 MB 792.3 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.9/11.0 MB 799.1 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.0/11.0 MB 800.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.0/11.0 MB 805.9 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.1/11.0 MB 798.4 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.1/11.0 MB 799.7 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.2/11.0 MB 804.8 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.2/11.0 MB 806.9 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.2/11.0 MB 811.8 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.3/11.0 MB 809.1 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.3/11.0 MB 806.5 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/11.0 MB 811.1 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/11.0 MB 813.0 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/11.0 MB 814.9 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.5/11.0 MB 810.8 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.5/11.0 MB 815.1 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.5/11.0 MB 812.6 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.6/11.0 MB 816.7 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.6/11.0 MB 817.5 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.7/11.0 MB 818.3 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.7/11.0 MB 819.1 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 2.8/11.0 MB 820.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.8/11.0 MB 820.8 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.8/11.0 MB 821.5 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 822.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 822.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 820.6 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 820.6 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 820.6 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 804.7 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.0/11.0 MB 801.6 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 799.8 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 803.3 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.2/11.0 MB 804.8 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.2/11.0 MB 805.0 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.3/11.0 MB 808.3 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.3/11.0 MB 806.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.3/11.0 MB 806.8 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.4/11.0 MB 807.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.4/11.0 MB 810.7 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.5/11.0 MB 808.9 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.5/11.0 MB 809.2 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.6/11.0 MB 812.2 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.6/11.0 MB 811.1 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.6/11.0 MB 811.1 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.6/11.0 MB 806.1 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.7/11.0 MB 813.0 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.8/11.0 MB 813.2 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.8/11.0 MB 814.2 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.8/11.0 MB 817.5 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.9/11.0 MB 815.5 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.9/11.0 MB 816.0 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.9/11.0 MB 817.0 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 4.0/11.0 MB 817.1 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 4.0/11.0 MB 819.7 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 4.1/11.0 MB 820.8 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.1/11.0 MB 820.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.2/11.0 MB 818.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.2/11.0 MB 819.2 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.2/11.0 MB 819.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.3/11.0 MB 822.1 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.3/11.0 MB 823.1 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.4/11.0 MB 821.0 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.4/11.0 MB 822.0 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.4/11.0 MB 820.6 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 821.5 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 822.5 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 822.4 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 823.3 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 823.7 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 824.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 824.5 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 825.4 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.8/11.0 MB 827.6 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.8/11.0 MB 826.2 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 826.2 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 827.0 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 825.7 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.0/11.0 MB 826.5 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.0/11.0 MB 828.6 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.1/11.0 MB 828.4 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.1/11.0 MB 827.1 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.1/11.0 MB 829.1 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 5.2/11.0 MB 831.2 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.2/11.0 MB 829.8 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.3/11.0 MB 829.7 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.3/11.0 MB 827.2 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.3/11.0 MB 827.2 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.4/11.0 MB 830.7 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.4/11.0 MB 829.4 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.5/11.0 MB 829.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.5/11.0 MB 830.0 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.6/11.0 MB 830.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.6/11.0 MB 830.6 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.6/11.0 MB 832.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.7/11.0 MB 831.2 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 5.7/11.0 MB 831.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.8/11.0 MB 832.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.8/11.0 MB 831.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.9/11.0 MB 831.2 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.9/11.0 MB 832.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 6.0/11.0 MB 833.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 6.0/11.0 MB 833.4 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.0/11.0 MB 833.8 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.1/11.0 MB 835.7 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.1/11.0 MB 832.4 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 834.5 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 835.1 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 834.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 835.6 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.3/11.0 MB 837.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 837.8 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 836.7 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 836.5 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.5/11.0 MB 837.1 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.5/11.0 MB 836.0 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 835.9 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.6/11.0 MB 836.5 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.6/11.0 MB 835.4 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.7/11.0 MB 836.9 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.7/11.0 MB 835.4 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.7/11.0 MB 837.3 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.8/11.0 MB 836.3 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.8/11.0 MB 837.7 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.9/11.0 MB 837.9 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.9/11.0 MB 838.2 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.0/11.0 MB 836.8 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.0/11.0 MB 837.0 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.0/11.0 MB 837.0 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.1/11.0 MB 838.9 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.1/11.0 MB 837.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.2/11.0 MB 839.3 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.2/11.0 MB 838.3 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.2/11.0 MB 838.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.3/11.0 MB 838.7 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.3/11.0 MB 840.1 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.4/11.0 MB 839.1 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.4/11.0 MB 838.1 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.4/11.0 MB 841.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.5/11.0 MB 840.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.5/11.0 MB 840.5 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.5/11.0 MB 840.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.6/11.0 MB 839.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.6/11.0 MB 839.8 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.7/11.0 MB 841.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.7/11.0 MB 841.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 841.2 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 841.3 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 840.4 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 841.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 841.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 834.3 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 836.3 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.0/11.0 MB 836.1 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.0/11.0 MB 836.0 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.0/11.0 MB 835.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 836.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 835.5 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 836.0 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 836.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 835.9 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.3/11.0 MB 836.5 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.3/11.0 MB 837.0 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.3/11.0 MB 837.0 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.4/11.0 MB 837.3 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.4/11.0 MB 836.8 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.5/11.0 MB 837.9 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.5/11.0 MB 837.1 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.6/11.0 MB 838.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.6/11.0 MB 838.7 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.6/11.0 MB 839.9 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.7/11.0 MB 837.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.7/11.0 MB 838.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.7/11.0 MB 838.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.8/11.0 MB 838.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.8/11.0 MB 837.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.9 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.9/11.0 MB 838.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.0/11.0 MB 825.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.0/11.0 MB 825.7 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.1/11.0 MB 825.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.1/11.0 MB 826.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.2/11.0 MB 826.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.2/11.0 MB 826.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.2/11.0 MB 825.8 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.3/11.0 MB 826.9 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 9.3/11.0 MB 827.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 826.2 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 826.4 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.5/11.0 MB 827.7 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.5/11.0 MB 827.9 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.5/11.0 MB 827.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.6/11.0 MB 827.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.6/11.0 MB 827.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 827.6 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 828.6 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 827.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.8/11.0 MB 827.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.8/11.0 MB 828.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.9/11.0 MB 828.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.9/11.0 MB 828.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.0/11.0 MB 829.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.0/11.0 MB 829.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.0/11.0 MB 829.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.1/11.0 MB 829.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.1/11.0 MB 830.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 10.1/11.0 MB 829.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 830.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 830.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.4/11.0 MB 837.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.4/11.0 MB 836.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.4/11.0 MB 835.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.6/11.0 MB 837.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.6/11.0 MB 835.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.0 MB 830.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/11.0 MB 833.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 833.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 823.2 kB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/301.8 kB 660.6 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/301.8 kB 787.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/301.8 kB 939.4 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 153.6/301.8 kB 833.5 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 194.6/301.8 kB 908.0 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 235.5/301.8 kB 850.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 256.0/301.8 kB 874.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 301.8/301.8 kB 889.0 kB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version 2.10.0\n",
      "Using keras version 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST Dataset import and preparation\n",
    "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_images :  (60000, 28, 28)\n",
      "Size of train_labels :  (60000,)\n",
      "Size of test_images :  (10000, 28, 28)\n",
      "Size of test_labels :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
    "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "print(\"Size of train_images : \",train_images.shape)\n",
    "print(\"Size of train_labels : \",train_labels.shape)\n",
    "print(\"Size of test_images : \",test_images.shape)\n",
    "print(\"Size of test_labels : \",test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Using the pyplot package, visualize the first sample of the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21aa33b7940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize the first training sample using the Matplotlib library with the imshow function\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(train_images[0],cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
    "Reminder: in order to do multi-class classification, we use the **softmax function**, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
    "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
    "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
    "Here, you need to turn train and test labels to one-hot encoding using the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels,num_classes=10)\n",
    "test_labels = to_categorical(test_labels,num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
    "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
    "\n",
    "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images to vectors of pixels\n",
    "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
    "train_images = train_images.reshape(train_images.shape[0],784)\n",
    "test_images = test_images.reshape(test_images.shape[0],784)\n",
    "\n",
    "\n",
    "# Cast pixels from uint8 to float32\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Now let us normalize the images so that they have zero mean and standard deviation\n",
    "train_mean = np.mean(train_images)\n",
    "train_std = np.std(train_images)\n",
    "train_images = (train_images - train_mean)/train_std\n",
    "test_images = (test_images - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Numpy\n",
    "\n",
    "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
    "\n",
    "### 3.1 Defining the model \n",
    "\n",
    "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
    "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
    "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
    "\n",
    "and our scores are obtained with:\n",
    "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
    "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
    "\n",
    "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid implementing a complicated gradient back-propagation,\n",
    "# we will try a very simple architecture with one layer \n",
    "def initLayer(n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the weights, return the number of parameters\n",
    "    Inputs: n_input: the number of input units - int\n",
    "          : n_output: the number of output units - int\n",
    "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
    "           : b: a vector bias for the layer - numpy ndarray\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create W at the right size with a normal distribution\n",
    "    W = np.random.normal(0.0,1.0,(n_input,n_output))\n",
    "    # Create b at the right size, with zeros\n",
    "    b = np.zeros((n_output))\n",
    "    nb_params = (n_input+1)*n_output\n",
    "    return W, b, nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = train_images.shape[0] \n",
    "n_feature = img_rows * img_cols\n",
    "n_labels = 10\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, b, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: W: the weights - numpy ndarray\n",
    "          : b: the bias - numpy ndarray\n",
    "          : X: the batch - numpy ndarray\n",
    "    Outputs: z: outputs - numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    z = W.T @ X + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing the output \n",
    "\n",
    "To obtain classification probabilities, we use the softmax function:\n",
    "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
    "\n",
    "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
    "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Perform the softmax transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - numpy ndarray\n",
    "    Outputs: out: the activation values - numpy ndarray\n",
    "    \"\"\"\n",
    "    z_mean = np.mean(z)\n",
    "    z_sum_exp = np.sum(np.exp(z-z_mean))\n",
    "    return np.exp(z-z_mean)/z_sum_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Making updates\n",
    "\n",
    "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
    "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} + \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
    "\n",
    "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
    "\n",
    "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
    "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
    "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
    "\n",
    "Coordinate by coordinate, we obtain the following update: \n",
    "$$ W_{ij}^{t+1} = W_{ij}^{t} + \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
    "\n",
    "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
    "\n",
    "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
    "\n",
    "We compute the softmax derivative, to obtain:\n",
    "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
    "\n",
    "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
    "\n",
    "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
    "\n",
    "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: param: the network parameters - ndarray\n",
    "          : grad_param: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : weight_decay: the weight-decay - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    if regularizer==None:\n",
    "        return param - eta*grad_param\n",
    "    elif regularizer=='L2':\n",
    "        return (1-2*weight_decay)*param - eta*grad_param\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : W: the weights - ndarray\n",
    "          : b: the bias -  ndarray\n",
    "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : X: the data -  ndarray\n",
    "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
    "          : weight_decay: the weight decay to be used in updateParams - float\n",
    "    Outputs: W: the weights updated -  ndarray\n",
    "           : b: the bias updated -  ndarray\n",
    "    \"\"\"\n",
    "    grad_w = X.reshape(X.shape[0],1) @ grad.reshape(grad.shape[0],1).T\n",
    "    grad_b = grad\n",
    "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
    "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Computing the Accuracy\n",
    "\n",
    "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcc(W, b, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    correct_preds = 0\n",
    "    for i,x in enumerate(X) :\n",
    "      # Forward propagation\n",
    "      z = forward(W,b,x)\n",
    "      \n",
    "      # Compute the softmax and the prediction\n",
    "      out = softmax(z)\n",
    "      pred = out.argmax()\n",
    "      correct_preds += labels[i][pred]\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = correct_preds/X.shape[0]\n",
    "      \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Preparing training\n",
    "\n",
    "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each training example once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "eta = 0.01\n",
    "regularizer = 'L2'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Training\n",
    "log_interval = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.06975 0.0663 0.01\n",
      "5000 0.83585 0.8401 0.01\n",
      "10000 0.8325833333333333 0.8402 0.01\n",
      "15000 0.8334666666666667 0.8392 0.01\n",
      "20000 0.84495 0.8478 0.01\n",
      "25000 0.8217166666666667 0.8312 0.01\n",
      "30000 0.7977833333333333 0.8008 0.01\n",
      "35000 0.7719 0.7707 0.01\n",
      "40000 0.8276833333333333 0.8276 0.01\n",
      "45000 0.8518833333333333 0.8549 0.01\n",
      "50000 0.8346833333333333 0.8401 0.01\n",
      "55000 0.8029166666666666 0.8008 0.01\n",
      "Final result: 0.8029166666666666 0.8008 0.01\n"
     ]
    }
   ],
   "source": [
    "# Data structures for plotting\n",
    "g_train_acc=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#######################\n",
    "### Learning process ##\n",
    "#######################\n",
    "\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)\n",
    "for j in range(n_training):\n",
    "    # Getting the example\n",
    "    X, y = train_images[j],train_labels[j]\n",
    "\n",
    "    # Forward propagation\n",
    "    z = forward(W,b,X)\n",
    "    \n",
    "    # Compute the softmax\n",
    "    out = softmax(z)\n",
    "        \n",
    "    # Compute the gradient at the top layer\n",
    "    \n",
    "    derror = out - y # This is o - y \n",
    "    \n",
    "    # Update the parameters\n",
    "    \n",
    "    W, b = update(eta, W, b, derror, X, regularizer, weight_decay)\n",
    "    \n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        # Every log_interval examples, look at the training accuracy\n",
    "        train_accuracy = computeAcc(W, b, train_images, train_labels) \n",
    "\n",
    "        # And the testing accuracy\n",
    "        test_accuracy = computeAcc(W, b, test_images, test_labels) \n",
    "\n",
    "        g_train_acc.append(train_accuracy)\n",
    "        g_valid_acc.append(test_accuracy)\n",
    "        result_line = str(int(j)) + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "        print(result_line)\n",
    "\n",
    "g_train_acc.append(train_accuracy)\n",
    "g_valid_acc.append(test_accuracy)\n",
    "result_line = \"Final result:\" + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "print(result_line)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoders with Keras\n",
    "### 4.1 Autoencoder and PCA\n",
    "\n",
    "First, we will try to connect the representation produced by Principal Component Analysis with what is learnt by a simple, linear, autoencoder. We will use the ```scikit-learn``` implementation of the ```PCA``` to obtain the two first components (hint: use the attribute ```.components_```), and visualize them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Second Principal Component')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's find the first 2 PCA components\n",
    "num_components = 2\n",
    "pca = PCA(n_components=num_components).fit(train_images)\n",
    "\n",
    "# Reshape so they resemble images and we can print them\n",
    "eigen_mnist = pca.components_.reshape(2,28,28)\n",
    "\n",
    "# Show the reshaped principal components\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(eigen_mnist[0], cmap='gray')\n",
    "ax[0].set_xlabel('First Principal Component')\n",
    "ax[1].imshow(eigen_mnist[1], cmap='gray')\n",
    "ax[1].set_xlabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09703726, 0.07094982], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the variance explained by those components\n",
    "pca.explained_variance_ratio_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
